
/**
 * Defines a standardized interface for integrating different Large Language Model providers.
 * This abstraction allows the application to switch between different LLM services
 * (like Claude, Gemini, Qwen, etc.) without modifying business logic.
 */
export interface LLMExecutor {
  /** 
   * Unique identifier for the LLM provider implementation.
   * Used for logging, metrics, and provider-specific configuration.
   */
  name: string;

  /**
   * Indicates whether this LLM implementation supports streaming token-by-token responses.
   * Affects UI feedback and response handling strategy.
   */
  supportsStreaming: boolean;

  /**
   * Executes a prompt against the LLM and returns the complete response.
   * @param prompt - The input text to send to the LLM
   * @returns A promise that resolves to the complete response string
   */
  executePrompt(prompt: string): Promise<string>;

  /**
   * Streams response tokens as they are generated by the LLM.
   * Optional method - only required if supportsStreaming is true.
   * @param prompt - The input text to send to the LLM
   * @returns An async generator yielding response tokens as they arrive
   */
  streamPrompt?(prompt: string): AsyncGenerator<string>;

  /**
   * Generates Mermaid diagram markup based on provided context.
   * Optional capability for LLMs that support structured diagram generation.
   * @param context - Contextual data needed to generate the diagram
   * @returns A promise resolving to Mermaid-compatible markup string
   */
  generateMermaid?(context: any): Promise<string>;
}
