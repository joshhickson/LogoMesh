# LogoMesh System Architecture (v2.7 - Cognitive Kernel & Ecosystem Ready)

**Date:** May 23, 2025
**Version:** 2.7 (Reflecting comprehensive design for Phase 1 foundations and future ecosystem)

## 1. Overview & Guiding Philosophy

LogoMesh is a modular, local-first **cognitive framework** designed to function as a **memory substrate and reasoning engine**. It enables the storage, organization, evolution, and interfacing of structured thought representations, aiming to become an "ontological operating system." Users ("souls") interact with their dynamic knowledge graph and AI agents through various embodiments (2D desktop shells, 3D/VR/AR immersive environments) and communication protocols.

The architecture is founded on principles of:

*   **Modularity & Composability:** Core functionalities are distinct, interoperable services. LM-Core is the foundational kernel, extensible via a robust plugin system.
*   **Local-First & Privacy-Centric:** Prioritizes local execution and user data ownership.
*   **Extensibility & Adaptability:** Designed for future growth, supporting new interaction modalities, AI capabilities, and data sources.
*   **Event-Driven & API-Centric:** Decoupled communication via a universal event bus and well-defined APIs.
*   **Cognitive Augmentation & Semantic Compression:** The system actively assists in reasoning and insight generation. It employs **"context expansion through semantic compression,"** structuring memory intentionally so that even models with limited context windows can operate over vast conceptual spaces.
*   **User-Centric Design ("Place" & "Flow"):** Interfaces aim for intuitive interaction, minimizing cognitive load, and fostering a sense of "place" to align with natural human cognition.

The system is architected into two primary layers, with an ecosystem of supporting frameworks:

1.  **LM-Core (LogoMesh Core Framework):** The foundational cognitive OS kernel.
2.  **SentienceShell (LM-HI - Human Interface Engine):** The advanced, optional human-centric interaction layer (🚧 `planned` for future phases).

## 2. Core Architectural Layers

### 2.1. LM-Core (LogoMesh Core Framework)

**Purpose:** LM-Core is the universal, application-agnostic engine responsible for the creation, management, persistence, querying, and AI-driven processing of interconnected knowledge structures. It serves as the **cognitive memory lattice** and provides foundational services for AI integration, plugin execution, and data portability.

**Phase 1 Implementation Focus:** TypeScript with Node.js/Express.js for the API server, and SQLite for primary data persistence. All modules are within the `/core`, `/contracts`, and `/server` directories planned for Phase 0/1.

**Module Implementation State Key (for Phase 1):**
*   ✅ `built` – Core functionality fully implemented as per Phase 1 scope.
*   🔄 `stubbed` – Interface and basic structure defined; minimal or mock implementation for Phase 1.
*   🚧 `planned` – Conceptually defined; implementation deferred to later phases. Interface stubs may exist in `@contracts`.

**Key Internal Modules & Responsibilities (Phase 1 Scope & State):**

*   **A. Data & Knowledge Management (The Memory Substrate):**
    *   **Contracts (`@contracts/`)**: ✅ `built`
        *   TypeScript interfaces defining core data entities (`Thought`, `Segment`, `Tag`, etc.), adapter contracts (`StorageAdapter`, `GraphStorageAdapter`, `LLMExecutor`, `PluginAPI`, `ThoughtExportProvider`), event payloads. Includes stubs for future schemas like `Task`, `Pipeline`, `VoiceSegment` (e.g., in `@contracts/plugins/sentienceShell/`).
        *   *Cognitive Framing:* These define the semantic atoms and relational grammar of the cognitive graph, enabling structured memory.
    *   **`IdeaManager` (`@core/IdeaManager.ts`)**: ✅ `built` (uses `SQLiteStorageAdapter`)
        *   Central service for CRUD on core entities. Uses `StorageAdapter`. Implements entity business logic (ID generation, timestamps). Emits lifecycle events. Handles basic filtering.
    *   **`StorageAdapter` Interface (`@contracts/storageAdapter.ts`)**: ✅ `built`
    *   **`SQLiteStorageAdapter` (`@core/storage/sqliteAdapter.ts`)**: ✅ `built`
        *   Phase 1 concrete implementation for SQLite. Manages SQL interactions and DTO <-> normalized schema mapping.
    *   **`GraphStorageAdapter` Interface (`@contracts/storage/graphStorageAdapter.ts`)**: 🔄 `stubbed`
        *   `MockGraphAdapter` (`@core/storage/graph/mocks/`): 🔄 `stubbed` for Phase 1, simulates graph operations.
    *   **`MeshGraphEngine` (`@core/services/meshGraphEngine.ts`)**: 🔄 `stubbed`
        *   *Cognitive Framing:* Acts as a **semantic lens** and initial **compression engine**. Interprets relationships to simulate graph traversal, clustering, and relevance scoring for context summarization.
        *   Phase 1: Stubs for `getRelatedThoughts`, `clusterThoughtsByTag`, `generateSummaryNode`. Uses `IdeaManager` and `MockGraphAdapter`.
    *   **`PortabilityService` (`@core/services/portabilityService.ts`)**: ✅ `built`
        *   Handles JSON import/export via `IdeaManager`.
    *   **Database Schema (`@core/db/schema.sql` & `initDb.ts`)**: ✅ `built` (SQLite)

*   **B. AI Orchestration & Tasking (The Reasoning & Execution Layer):**
    *   **`LLMExecutor` Interface (`@contracts/llmExecutor.ts`)**: ✅ `built`
    *   **Concrete `LLMExecutor` Implementations (e.g., `@core/llm/MockLLMExecutor.ts`, stub for `@core/llm/OllamaExecutor.ts`)**: 🔄 `stubbed`
    *   **`LLMTaskRunner` (`@core/llm/LLMTaskRunner.ts`)**: ✅ `built` (for mock/simple execution)
        *   Uses injected `LLMExecutor`, logs via `llmAuditLogger`. Streaming-ready design.
    *   **`LLM Audit Logger` (`@core/logger/llmAuditLogger.ts`)**: ✅ `built` (Console logging)
    *   **`MermaidAuditor` (`@core/llm/utils/mermaidAuditor.ts`)**: 🔄 `stubbed`
    *   **`Task` & `Pipeline` Schemas (`@contracts/tasks.ts` or `@contracts/plugins/sentienceShell/taskPipeline.ts`)**: 🔄 `stubbed` (schema definitions)
    *   **`TaskEngine` (`@core/services/taskEngine.ts`)**: 🔄 `stubbed`
        *   Phase 1: Basic structure to manage `Task`/`Pipeline` definitions (from contracts). Implements `executePipeline(pipelineId)` using `ExecutorRegistry` with a `MockExecutor`. Emits lifecycle events.
    *   **`ExecutorRegistry` (part of `TaskEngine` or `PluginAPI`)**: 🔄 `stubbed`
        *   Allows registration/retrieval of named `TaskExecutor`s (including `LLMExecutor`s with labels like `fast-summary`).
    *   **`MetaExecutor.ts` (`@core/services/metaExecutor.ts`)**: 🔄 `stubbed`
        *   Intelligent agent to receive high-level tasks, consult `CognitiveLoadProfile` and `ModelSelector`, and route sub-tasks.
    *   **`ModelSelector.ts` (`@core/utils/modelSelector.ts` or service)**: 🔄 `stubbed`
        *   Selects LLM based on task, profiles, and registry. Used by `MetaExecutor`.
    *   **`IntentRouter.ts` (`@core/services/intentRouter.ts` or as a SentienceShell plugin contract)**: 🚧 `planned` (conceptual seam defined)
        *   Interface/concept for mapping high-level user/plugin intents to `TaskEngine` or `LLMExecutor` calls. Key for "Intent -> Memory Binding."
    *   **`TaskCostMetadata.ts` (`@contracts/config/taskCostMetadata.ts`)**: 🔄 `stubbed` (type definition)
        *   Optional metadata for tasks/plugins to declare cognitive "cost."

*   **C. Extensibility, State, API & System Services (The Cognitive OS Kernel):**
    *   **`EventBus` (`@core/services/eventBus.ts`)**: ✅ `built` (in-memory EventEmitter)
        *   Universal signal router. Supports typed, namespaced events. Defines core Phase 1 event types.
    *   **`PluginHost` (`@core/services/pluginHost.ts`)**: 🔄 `stubbed`
        *   Manages plugin lifecycle. Phase 1: Static registration from manifest, provides `PluginAPI`. Basic capability validation.
    *   **`PluginAPI` (`@contracts/plugins/pluginApi.ts`)**: 🔄 `stubbed`
        *   Stable interface for plugins to LM-Core services.
    *   **`PluginManifest` Schema (`@contracts/plugins/pluginManifest.schema.json`)**: 🔄 `stubbed`
        *   Defines plugin metadata: `version`, `dependencies`, `capabilities` (e.g., `"canChangeContext": true`).
    *   **`ContextStateService` (`@core/state/contextStateService.ts`)**: ✅ `built`
        *   Tracks `activeContextId`, `activeFocusId`, `activeMode`. Observable and writable via `PluginAPI`. Emits events. Represents system's **semantic focus**.
    *   **`ExecutionStateService` (`@core/state/executionStateService.ts`)**: 🔄 `stubbed`
        *   Tracks status of active `Pipelines` and `Tasks` (e.g., `activePipelineIds[]`, `lastCompletedPipeline`). Observable, emits events. Provides a "dashboard" for ongoing cognitive work.
    *   **`CognitiveProfileService.ts` (`@core/services/cognitiveProfileService.ts`)**: 🔄 `stubbed`
        *   Determines/exposes current `CognitiveLoadProfile`. Phase 1: Uses hardcoded/config setting.
    *   **`CognitiveLoadProfile.ts` (`@contracts/config/cognitiveLoadProfile.ts`)**: 🔄 `stubbed`
        *   Defines profiles (e.g., concurrent LLM limits).
    *   **`ThoughtExportProvider` Interface (`@contracts/thoughtExportProvider.ts`)**: ✅ `built` (interface), Implementations in `IdeaManager`/`MeshGraphEngine`: 🔄 `stubbed` (for advanced filtering/compression)
        *   Contract for exporting structured, filtered, and **semantically compressed** slices of the ThoughtGraph.
        *   Phase 1 implementation supports key filtering options (`tagFilter`, `clusterIds`, `maxDepth` (simulated), `abstractionLevel` reference, `localPriorityThreshold`).
        *   *Cognitive Framing:* Functions as a **semantic compression engine** for context-relevant summaries.
    *   **Backend API Server (`server/`)**: ✅ `built`
        *   Node.js/Express.js. Exposes RESTful API (`/api/v1/...`) for LM-Core services.
    *   **Utilities (`@core/utils/`)**: ✅ `built`
        *   `idUtils.ts`, `logger.ts`.

### 2.2. SentienceShell (LM-HI - Human Interface Engine)

**Purpose:** 🚧 `planned` (Not built in Phase 1)
An optional, advanced layer for rich, multimodal, context-aware, conversational human interaction.

**Phase 1 Impact on LM-Core:** LM-Core is scaffolded to support SentienceShell's future integration via `EventBus` (e.g., `shell/intentReceived` event type defined), `PluginHost`, `TaskEngine`, `ContextStateService`, and defined schemas for `VoiceSegment` etc. in `@contracts/plugins/sentienceShell/`. The `IntentRouter` concept is acknowledged as the bridge.

### 2.3. "Moon" & "Satellite" Frameworks (Conceptual Plugins/Extensions)

All are 🚧 `planned` for future phases and will integrate as plugins. LM-Core Phase 1 provides the `PluginHost` and `PluginAPI` for them. Examples: `Logosync`, `Logoweave`, `BruteForce Core`, `MetaPlugins Core` (advanced plugin management), `HemiSync Framework`, `EchoMesh`, `MatrixCore` (VR/AR - C#/Unity), `ShellNode` (advanced OS control - TS/Electron, Rust/Janet).

## 3. Data Persistence Strategy (Phase 1)

*   **Primary Structured Data:** SQLite (via `SQLiteStorageAdapter`). Normalized relational schema.
*   **Conceptual Graph Data:** Implicitly represented in SQL relations; traversed/simulated by `MeshGraphEngine`. `GraphStorageAdapter` stubbed for future dedicated Graph DB (e.g., Memgraph/Cypher).
*   **AI Task/Pipeline Definitions:** Schemas 🔄 `stubbed`. Storage via `IdeaManager` as generic JSON objects or managed by future plugins.
*   **Logs:** Console (Phase 1). LLM interactions via `llmAuditLogger`.
*   **Configuration:** JSON files (`config.json` stub for profiles/plugins), environment variables.
*   **Data Interchange Format:** JSON.

## 4. Technology Stack Summary (Phase 1 LM-Core & Demo)

*   **LM-Core & Backend API:** TypeScript, Node.js, Express.js
*   **Database:** SQLite (SQL)
*   **Frontend Demo:** React (JavaScript/JSX with TypeScript types)
*   **Graph Viz (Demo):** Cytoscape.js
*   **Local Automation:** Node-RED (via HTTP API)
*   **Containerization:** Docker, Docker Compose (API + DB)
*   **Testing:** Jest, Supertest

## 5. Key Abstraction Boundaries & Interfaces for Phase 1 (Reiteration)

*   `StorageAdapter` / `GraphStorageAdapter` (mock)
*   `LLMExecutor` / `ExecutorRegistry` (with `MockExecutor`)
*   `PluginAPI` / `PluginHost` (with `PluginManifest` schema)
*   `EventBus` (typed, namespaced events)
*   `ThoughtExportProvider` (with semantic compression intent)
*   Backend REST API (`/api/v1/`)
*   `@contracts/*` (Canonical DTOs, service interfaces, config types)
*   `ContextStateService` & `ExecutionStateService` (observable state)

## 6. System Lifecycle & Developer Experience (Phase 1 Foundations)

*   **Error Handling & Recovery:** 🔄 `stubbed` (Basic `onError` event patterns for critical services; task fallbacks are 🚧 `planned`).
*   **Session Persistence (`session.json` concept):** 🚧 `planned` (`ContextStateService` is in-memory; future plugins might persist/restore).
*   **Execution Sandbox Stubs:** 🚧 `planned` (Placeholders in `TaskEngine`/`MetaExecutor` for future timeout/resource limits).
*   **Version Tagging:** ✅ `built` (for schema in manifests/core modules), 🔄 `stubbed` (for runtime versioning process).
*   **Developer Workflow:**
    *   `plugin-dev-template/`: 🔄 `stubbed` (Directory and basic file stubs to be created).
    *   `local-dev-config.md`: 🔄 `stubbed` (Document to be created outlining local setup).
    *   Developer CLI Bootstrap (`logomesh dev`, etc.): 🚧 `planned` (API will support actions these CLIs would trigger).
*   **Installer/Bootstrap Vision:** 🚧 `planned` (Phase 1 lays groundwork with config files & modular plugin loading).
*   **Input Templating System:** 🚧 `planned` (System to transform structured inputs like web forms into `ThoughtNodes`).
*   **DevShell Plugin Concept:** 🚧 `planned` (Future embedded dev environment in ShellNode).

## 7. LogoMesh Context Manifesto Integration

The principle of **“Context expansion through semantic compression”** is a core driver for LM-Core's design. `ThoughtExportProvider`, `MeshGraphEngine`, and `ContextStateService` are architected in Phase 1 to structure and retrieve knowledge as focused, semantically rich slices. This enables downstream consumers (LLMs, UIs, plugins) to operate on relevant information without being overwhelmed by the raw data volume, effectively simulating a larger working memory and enabling more nuanced reasoning. Phase 1 implements the foundational mechanisms for this data shaping and retrieval.

This architecture establishes LM-Core in Phase 1 not merely as a CRUD application with an API, but as a **foundational memory substrate and cognitive OS kernel**, designed for extensibility, AI-native operations, and sophisticated human-computer interaction in subsequent phases.