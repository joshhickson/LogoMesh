---
status: SNAPSHOT
type: Research
---
> **Context:**
> * [2025-12-17]: Historical snapshot of early theory. Commercial strategies PAUSED.

# **The Unknowable Code: A Framework for Measuring and Mitigating Software Liability**

Abstract  
We identify and formalize Contextual Debt, a growing class of software liability defined as the decoupling of executable logic from human intent. While traditional Technical Debt describes the cost of suboptimal implementation, Contextual Debt describes the loss of architectural rationale and domain purpose, resulting in systems that are functionally opaque and increasingly hazardous to modify. This phenomenon is accelerating due to the divergence between the velocity of AI-based code generation ($\\lambda$) and the capacity of human review ($\\mu$). We propose a novel governance architecture, the "Glass Box" Protocol, and a quantitative metric, the Contextual Integrity Score (CIS), to restore determinism. By externalizing the reasoning process of AI agents into a Decision Bill of Materials (DBOM), we demonstrate a method for maintaining system auditability and satisfying the emerging legal "duty of care" standards for autonomous software systems.

## **1\. Introduction: The Physics of Contextual Debt**

Modern software engineering is witnessing a fundamental phase transition. For two decades, the industry has managed **Technical Debt**—a deliberate, prudent tradeoff where suboptimal implementation is chosen to accelerate delivery.1 This debt is a failure of the *how*; it makes systems difficult to maintain. However, the proliferation of generative AI and fragmented microservice architectures is introducing a new, non-linear risk factor: **Contextual Debt**.

We define **Contextual Debt ($D$)** as the accumulated liability resulting from the loss of discernible human intent, architectural rationale, and domain-specific knowledge within a codebase. Unlike technical debt, which is a failure of implementation, Contextual Debt is a failure of the *why*. It manifests as "amnesiac systems"—codebases that function but contain no recoverable record of the business rules or safety constraints they enforce.

### **1.1 The Decoupling of Intent and Execution**

In traditional development, the "theory" of the program—the mental model of why it works—resides in the minds of the developers.2 When code is generated by Large Language Models (LLMs), this theory is never formed in a human mind. The AI generates a solution based on a transient probabilistic state, and unless explicitly captured, the "why" evaporates at the moment of generation.

This decoupling introduces a new form of liability. A system laden with Contextual Debt is not merely expensive to change; it is dangerous to touch. Modifications risk violating unstated, "unknowable" constraints, leading to catastrophic failures in security, compliance, or data integrity. As regulatory frameworks shift from strict liability to a negligence standard based on a "duty of care,"3 the inability to explain the rationale behind system behavior becomes a quantifiable legal risk.

## **2\. The Mathematical Necessity of Automated Governance**

The accumulation of Contextual Debt is not an artifact of poor discipline but a deterministic outcome of current velocity differentials.

### **2.1 The Decay Theorem**

Let $V\_g$ be the velocity of code generation and $V\_r$ be the velocity of effective human review. In the pre-AI era, $V\_g \\approx V\_r$. The adoption of AI coding assistants has decoupled these variables, such that $V\_g \= \\lambda$ and $V\_r \= \\mu$, where $\\lambda \\gg \\mu$.

We model the probability $P$ that a system remains in a "Knowable State" (where intent matches execution) over time $t$ as a function of this divergence. If code is generated faster than it can be comprehended and contextualized by humans, the probability of maintaining Contextual Integrity decays exponentially:

$$P(\\text{Knowable})\_t \\approx e^{-(\\lambda \- \\mu)t}$$  
As $t \\to \\infty$, $P \\to 0$. This implies that reliance on human-in-the-loop review alone is mathematically insufficient to prevent the total loss of system intent. To maintain $P \\approx 1$, the system requires a governance mechanism where the validation velocity scales linearly with generation velocity. This necessitates the **Agent-as-a-Judge** architecture defined in Section 4\.

## **3\. Formalization: The Contextual Integrity Score (CIS)**

To quantify this risk, we move beyond subjective "code quality" metrics. We define the **Contextual Integrity Score (CIS)** as a computable probability that a given software artifact $\\Delta$ preserves its original intent. The CIS is a weighted summation of three vector-space measurements:

$$CIS(\\Delta) \= w\_r \\cdot R(\\Delta) \+ w\_a \\cdot A(\\Delta) \+ w\_t \\cdot T(\\Delta)$$  
Where $w\_r, w\_a, w\_t$ are organizational risk weights summing to 1\.

### **3.1 Rationale Integrity ($R$): The Semantic Alignment Vector**

Rationale Integrity measures the semantic density between the executable logic and its declarative intent. Let $\\vec{v}\_{code}$ be the vector embedding of the code change and $\\vec{v}\_{intent}$ be the aggregated vector embedding of the associated requirements (tickets, PRDs, ADRs).

$$R(\\Delta) \= \\frac{\\vec{v}\_{code} \\cdot \\vec{v}\_{intent}}{\\|\\vec{v}\_{code}\\| \\|\\vec{v}\_{intent}\\|} \\cdot \\mathbb{I}(Link\_{exists})$$  
Where $\\mathbb{I}$ is an indicator function returning 0 if no formal link exists. This metric mathematically assesses whether the code implements the *meaning* of the requirement, not just its syntax.

### **3.2 Architectural Integrity ($A$): The Graph Centrality Constraint**

We model the software architecture as a directed graph $G \= (V, E)$, where $V$ represents services and $E$ represents allowed dependencies. Contextual Debt often manifests as "illegal edges"—hidden dependencies that violate architectural constraints (e.g., a frontend service querying a payment database directly).

For a proposed change $\\Delta$ introducing a set of new edges $E\_{new}$, the Architectural Integrity is calculated as:

$$A(\\Delta) \= \\prod\_{e \\in E\_{new}} (1 \- P(Violation | e))$$  
This function acts as a "Critical Veto," forcing the score to 0 if a single critical architectural boundary is violated.

### **3.3 Testing Integrity ($T$): Semantic Coverage**

Traditional coverage metrics measure execution, not verification. We define Testing Integrity ($T$) as the semantic overlap between the Test Case assertions ($\\vec{v}\_{test}$) and the Acceptance Criteria ($\\vec{v}\_{criteria}$):

$$T(\\Delta) \= \\text{Coverage}(\\Delta) \\times \\text{Sim}(\\vec{v}\_{test}, \\vec{v}\_{criteria})$$  
This penalizes "vanity metrics," ensuring that tests actually validate the stated business goals.

## **4\. The Protocol: Agent-as-a-Judge Architecture**

To enforce the CIS, we propose the **"Glass Box" Protocol**. Unlike "Black Box" monolithic agents, this architecture externalizes reasoning and cryptographically verifies intent.

### **4.1 The Orchestrator-Worker Topology**

We utilize a **Multi-Agent Orchestrator-Worker (MA-OW)** topology to ensure auditability:

* **The Orchestrator ($O$):** Maintains the state of the global context graph $G$. It generates constraints, not code.  
* **The Worker ($W$):** Specialized agents (e.g., Payment\_Worker) that generate code $\\Delta$ strictly to satisfy constraints passed by $O$.  
* **The Judge ($J$):** An adversarial agent that evaluates $\\Delta$ against the $CIS$ function.

The system accepts a change $\\Delta$ if and only if:

$$J(O(C), W(\\Delta)) \\to \\{Accept \\mid Reject\\}$$

### **4.2 The Decision Bill of Materials (DBOM)**

To satisfy the legal requirement for explainability, the system generates a **Decision Bill of Materials (DBOM)**. While an SBOM lists components, a DBOM tracks causality. For every commit $\\Delta\_i$, the DBOM records an immutable tuple:

$$DBOM\_i \= \\langle H(\\Delta\_i), \\vec{v}\_{intent}, \\text{Score}\_{CIS}, \\sigma\_{Judge} \\rangle$$  
Where:

* $H(\\Delta\_i)$ is the SHA-256 hash of the code change.  
* $\\vec{v}\_{intent}$ is the vector embedding of the "Why" (rationale).  
* $\\text{Score}\_{CIS}$ is the computed Contextual Integrity Score.  
* $\\sigma\_{Judge}$ is the digital signature of the Agent-as-a-Judge.

This creates a verifiable audit trail linking every line of code to its justification.

### **4.3 Adversarial Context Defense**

A critical vulnerability in automated governance is "Context Stuffing"—where an agent hallucinates irrelevant justification to artificially inflate its Rationale Integrity score ($R$).

To mitigate this, the Judge agent employs a **Semantic Drift Detection** filter. It calculates the Kullback-Leibler (KL) divergence between the *Requirement Distribution* ($P$) and the *Code Implementation Distribution* ($Q$). If the divergence exceeds a threshold $\\theta$:

$$D\_{KL}(P \\parallel Q) \> \\theta \\implies \\text{Reject}(\\Delta)$$  
This enforces a "semantic bounding box," mathematically preventing the agent from citing irrelevant context to game the system.

## **5\. Conclusion: From Debt to Asset**

The era of "move fast and break things" relied on the assumption that code could be fixed faster than the debt accrued. The introduction of generative AI invalidates this assumption by increasing the rate of debt accumulation ($\\lambda$) beyond the human capacity for remediation ($\\mu$).

Contextual Debt is the defining liability of the next decade of software. It cannot be solved by hiring more humans; it must be solved by better architecture. The **Glass Box Protocol** and the **Contextual Integrity Score** provide the necessary mathematical and structural framework to manage this risk. By treating intent as a first-class, computable citizen, organizations can transition from maintaining "unknowable code" to building defensible, transparent, and resilient systems.

## **Works Cited**

1 Cunningham, W. (1992). "The WyCash Portfolio Management System." OOPSLA '92 Experience Report.  
4 "What is Technical Debt? Definition, History, and Strategy." Praxent.  
5 Callaou, Y. (2025). "Our Systems Have No Memory. I Call It Contextual Debt." Medium.  
2 "Why AI-Generated Code Hurts Your Exit." The Bootstrapped Founder.  
6 Verdecchia, R. (2023). "Tracing the Footsteps of Technical Debt in Microservices."  
3 "The Future of Software Liability." Veritas.  
7 "How AI-Generated Code is messing with your Technical Debt." Kodus.  
8 "ADR process." AWS Prescriptive Guidance.  
"Understanding Domain-Driven Design." SensioLabs.

#### **Works cited**

1. \[2506.18096\] Deep Research Agents: A Systematic Examination And Roadmap \- arXiv, accessed November 19, 2025, [https://arxiv.org/abs/2506.18096](https://arxiv.org/abs/2506.18096)  
2. Our Techniques for Building LLM-Powered Autonomous Agents | Width.ai, accessed November 19, 2025, [https://www.width.ai/post/llm-powered-autonomous-agents](https://www.width.ai/post/llm-powered-autonomous-agents)  
3. PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving \- arXiv, accessed November 19, 2025, [https://arxiv.org/pdf/2502.16111?](https://arxiv.org/pdf/2502.16111)  
4. MCP-Agent: How to Build Scalable Deep Research Agents | AI ..., accessed November 19, 2025, [https://thealliance.ai/blog/building-a-deep-research-agent-using-mcp-agent](https://thealliance.ai/blog/building-a-deep-research-agent-using-mcp-agent)  
5. ReAct, Tree-of-Thought, and Beyond: The Reasoning Frameworks Behind Autonomous AI Agents \- Coforge, accessed November 19, 2025, [https://www.coforge.com/what-we-know/blog/react-tree-of-thought-and-beyond-the-reasoning-frameworks-behind-autonomous-ai-agents](https://www.coforge.com/what-we-know/blog/react-tree-of-thought-and-beyond-the-reasoning-frameworks-behind-autonomous-ai-agents)  
6. \[NeurIPS 2023\] Large Language Model-Based Autonomous Agents \- LG AI Research BLOG, accessed November 19, 2025, [https://www.lgresearch.ai/blog/view?seq=409](https://www.lgresearch.ai/blog/view?seq=409)  
7. SCORE: Story Coherence and Retrieval Enhancement for AI Narratives \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2503.23512v2](https://arxiv.org/html/2503.23512v2)  
8. Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents \- arXiv, accessed November 19, 2025, [https://arxiv.org/html/2502.17321v1](https://arxiv.org/html/2502.17321v1)