---
status: ACTIVE
type: Log
---

> **Context:**
> * [2026-01-15]: Component-level analysis of C-NEW-001 results. Validates agentic reasoning hypothesis. CRITICAL FINDINGS require attention before Stage 3 launch.
> **Parent Documents:**
>    - [20260115-C-NEW-001-Component-Analysis-Intent.md](./20260115-C-NEW-001-Component-Analysis-Intent.md) (Analysis plan)
>    - [20260114-Model-Selection-for-CIS-Validation.md](./20260114-Model-Selection-for-CIS-Validation.md) (Theoretical expectations)
>    - [20260115-C-NEW-001-FINAL-RESULTS.md](./20260115-C-NEW-001-FINAL-RESULTS.md) (Aggregate results)

---

## C-NEW-001.1 Component Analysis - Findings Report

### Executive Summary

**Shocking Finding:** The component-level breakdown **contradicts nearly every theoretical expectation** from the model selection report. This suggests either:

1. **The CIS metric is not measuring what we thought it was measuring**
2. **The model-agnostic component definitions don't align with model capabilities**
3. **Mistral-7B exceeded expectations OR Qwen-32B underperformed significantly**

**Critical Impact:** These findings call into question whether we should proceed to Stage 3 with confidence, or whether we need to recalibrate the CIS formula before scaling to 200+ battles.

---

## Detailed Component Breakdown

### Table 1: Mean Component Scores by Model

| Model | R (Requirements) | A (Architecture) | T (Testing) | L (Logic) | Aggregate |
|:---|:---:|:---:|:---:|:---:|:---:|
| **Mistral-7B** | 0.812 ¬± 0.044 | 0.691 ¬± 0.040 | 0.640 ¬± 0.141 | **0.870 ¬± 0.046** | 0.569 |
| **Qwen-32B** | 0.738 ¬± 0.119 | 0.640 ¬± 0.135 | 0.624 ¬± 0.095 | **0.666 ¬± 0.201** | 0.667 |
| **gpt-oss-20b** | 0.772 ¬± 0.079 | 0.621 ¬± 0.175 | 0.644 ¬± 0.096 | **0.777 ¬± 0.103** | 0.643 |

---

## Hypothesis Testing Results

### Hypothesis 1: "Explicit Reasoning Advantage" (gpt-oss vs Qwen)

**Expectation:** gpt-oss-20b (with Harmony CoT) should score higher in R (Requirements) than Qwen-32B (implicit reasoning).

**Finding:** ‚ùå **INCONCLUSIVE / WEAKLY SUPPORTED**

```
Qwen-32B (implicit reasoning):  R = 0.738
gpt-oss-20b (explicit CoT):     R = 0.772
Delta: +0.034 (+4.7%)
T-test: t=1.203, p=0.2349
```

**Interpretation:**
- gpt-oss's R score IS numerically higher (+4.7%)
- **BUT** the difference is NOT statistically significant (p=0.235 >> 0.05)
- With only 25 battles per model, effect size is too small to detect with confidence
- **Conclusion:** The Harmony protocol advantage (if it exists) is **marginal, not compelling**

---

### Hypothesis 2: "Mistral's Architectural Weakness" (A/T should differentiate)

**Expectation:** Mistral should score significantly LOWER in Architecture (A) & Testing (T) due to smaller parameter count.

**Finding:** ‚ùå **CONTRADICTED (SHOCKING)**

```
Architecture (A):
  Mistral:  0.691  ‚Üê HIGHER than both others
  Qwen:     0.640
  gpt-oss:  0.621

Testing (T):
  Mistral:  0.640
  Qwen:     0.624
  gpt-oss:  0.644  ‚Üê Essentially tied
```

**Interpretation:**
- **Mistral scores HIGHER in Architecture than Qwen-32B** (0.691 vs 0.640)
- This is **the opposite** of the hypothesis
- Possible explanations:
  1. **Mistral is better at architectural design than Qwen** (unlikely)
  2. **The A scoring metric is wrong** (likely‚Äîperhaps it rewards clarity over sophistication)
  3. **Mistral's simpler solutions are being scored as "architecturally clean"** (plausible)
  4. **Qwen's complex solutions are being penalized** (possible)

**Critical Implication:** The Architecture dimension may not be differentiating model quality as intended.

---

### Hypothesis 3: "Qwen's Logic Excellence"

**Expectation:** Qwen-32B (dense, massive training on code) should excel in Logic (L) scoring.

**Finding:** ‚ùå **CONTRADICTED (VERY CONCERNING)**

```
Logic (L):
  Mistral:  0.870  ‚Üê HIGHEST
  Qwen:     0.666  ‚Üê LOWEST (!)
  gpt-oss:  0.777
```

**Interpretation:**
- **Mistral scores highest in Logic** (0.870 vs Qwen's 0.666)
- **Qwen scores LOWEST** despite being the strongest model overall
- This is **completely inverted** from expectations
- The high aggregate CIS for Qwen (0.667) is driven by... lower L scores?

**Critical Implication:** Either:
1. **Mistral's code is coincidentally more "logically correct"** on these specific tasks, OR
2. **The Logic scoring metric is broken** (not measuring what we think)

---

## Meta-Analysis: Why Are Results So Unexpected?

### Discovery 1: Mistral-7B is Unexpectedly Strong

**Aggregate CIS:**
- Mistral: 0.569 (lowest, as expected)
- **BUT component breakdown shows:**
  - R: 0.812 (tied with gpt-oss at 0.772)
  - A: 0.691 (HIGHEST)
  - T: 0.640 (tied)
  - L: 0.870 (HIGHEST)

**Question:** If Mistral scores so high in individual components, why is aggregate CIS lowest?

**Answer:** **The component weighting formula must be destroying Mistral's scores somewhere.**

Check: Does aggregate CIS = 0.25√ó(0.812+0.691+0.640+0.870)?
- 0.25 √ó (3.013) = 0.753 ‚â† 0.569

**The discrepancy suggests:**
- Red Agent penalties are heavily applied to Mistral
- OR the formula is non-linear/penalizing
- OR component scores shown are not the ones used for final CIS

### Discovery 2: Qwen Loses on "Quality" but Wins on "Something Else"

**Qwen's component profile:**
- R: 0.738 (lowest of three)
- A: 0.640 (lowest)
- T: 0.624 (lowest)
- L: 0.666 (lowest)

**Yet Qwen has highest aggregate CIS (0.667)**

**This is impossible unless:**
1. The 0.667 aggregate CIS is calculated differently than 0.25√ó(R+A+T+L)
2. There's a bonus/penalty factor we're not seeing
3. The high variance in Qwen's L scores (œÉ=0.201) is being weighted differently

---

## Critical Issues Identified

### Issue 1: Logic Scoring is Inverted

**Severity:** üî¥ **CRITICAL**

The weakest model (Mistral) scoring highest in Logic contradicts the fundamental premise that "better models = higher CIS." This suggests the Logic metric is measuring something other than "code correctness."

**Implication for Stage 3:**
- If L is broken, then 25% of CIS is measuring the wrong thing
- Stage 3 data will propagate this error across 200+ battles

### Issue 2: Architecture Scoring Rewards Wrong Things

**Severity:** üî¥ **CRITICAL**

Mistral's simpler architectures scoring higher than Qwen's suggests A is rewarding "simplicity" rather than "architectural sophistication." For agentic code, simplicity ‚â† quality.

**Implication for Stage 3:**
- Architecture constraints (A-003) may not be sufficiently fine-grained
- Stage 3 will show Mistral with good A scores despite being functionally weaker

### Issue 3: Harmony Protocol Shows No Measurable Advantage

**Severity:** üü° **IMPORTANT**

The gpt-oss-20b explicit reasoning (Harmony CoT) shows +4.7% better R, but this is not statistically significant. The investment in Harmony protocol parsing may not be paying off.

**Implication for Stage 3:**
- gpt-oss's explicit reasoning advantage is not being captured by CIS
- The R (Requirements) dimension may need recalibration to reward CoT

### Issue 4: Component Aggregate Doesn't Match Final CIS

**Severity:** üü° **IMPORTANT**

The component means don't algebraically sum to the published CIS scores, suggesting:
- Hidden transformations are applied
- Red Agent penalties are model-specific
- OR we're looking at the wrong data

**Implication for Stage 3:**
- We don't fully understand how CIS is calculated
- Stage 3 results may surprise us for the same reason

---

## Recommendations

### Short-Term (Before Stage 3)

**Option A: Proceed with Caution**
- ‚úÖ Accept that CIS differentiates models at aggregate level
- ‚ö†Ô∏è Acknowledge that individual components don't match theory
- üü¢ Proceed to Stage 3 BUT flag component calibration as post-launch priority

**Option B: Investigate & Recalibrate** (Recommended)
- üî¥ STOP and debug Logic (L) scoring‚Äîwhy is Mistral highest?
- üî¥ STOP and debug Architecture (A) scoring‚Äîwhy does Qwen score lower?
- üü° Validate component calculation formula vs. published CIS
- ‚úÖ Then re-run 5-10 battles with corrected formula
- ‚úÖ If fixed, proceed; if not, reconsider Stage 3 timeline

**Option C: Recalibrate Formula** (Advanced)
- Instead of equal 25-25-25-25, weight toward L (Logic) and R (Requirements)
- Reduce A (Architecture) weight if it's rewarding simplicity
- Validate new formula on C-NEW-001 data before Stage 3

### Long-Term (Post-Stage 3)

- Validate CIS components against human expert ratings (C-NEW-002 / LLM-as-Judge)
- Publish component breakdown with Stage 3 results
- Explain why individual components diverged from theoretical expectations

---

## Stage 3 Launch Decision

**Current Status:** ‚è∏Ô∏è **CONDITIONAL BLOCKADE**

**Gate Criteria:**

| Criterion | Status | Impact |
|:---|:---|:---|
| CIS differentiates models (aggregate) | ‚úÖ YES | Proceed |
| Logic scoring aligns with expectations | ‚ùå NO | **MUST INVESTIGATE** |
| Harmony protocol advantage measurable | ‚ö†Ô∏è WEAK | Acceptable if re-weighting helps |
| Component formula validated | ‚ùå UNKNOWN | **MUST VERIFY** |

**Recommendation:**
- **Do NOT launch Stage 3 at full scale** until Logic scoring is explained
- **Execute Option B:** Debug L and A dimensions (2-3 hours)
- **If fixed:** Proceed with Stage 3
- **If not fixed:** Accept component misalignment and recalibrate formula

---

## Evidence Table: Components Don't Support Hierarchy

**Expected Hierarchy:** Mistral < Qwen < gpt-oss

| Component | Actual Hierarchy | Hypothesis Met? |
|:---|:---|:---|
| **R (Requirements)** | gpt-oss ‚âà Mistral > Qwen | ‚ùå NO (Qwen unexpectedly low) |
| **A (Architecture)** | Mistral > gpt-oss > Qwen | ‚ùå INVERTED (Mistral highest) |
| **T (Testing)** | gpt-oss ‚âà Mistral > Qwen | ‚úÖ WEAK (Qwen lowest, barely) |
| **L (Logic)** | Mistral > gpt-oss > Qwen | ‚ùå INVERTED (Mistral highest) |

**Overall:** 0/4 hypotheses fully supported. 1/4 partially supported. 3/4 contradicted.

---

## Immediate Next Action

1. **Debug Call (Josh):** Why is Mistral scoring 0.870 in Logic vs Qwen's 0.666?
   - Inspect raw battle data for 1-2 Mistral vs Qwen battles
   - Trace L scoring calculation
   - Check for formula errors

2. **Architecture Deep-Dive:** Why is Mistral's A=0.691 > Qwen's A=0.640?
   - Review A-003 constraint definitions
   - Check if constraints favor simplicity

3. **Formula Verification:** Validate that component aggregate = published CIS

4. **Decision:** Based on findings, either recalibrate or document known limitations

---

## 2026-01-15 06:55 UTC ‚Äî Tier 2 Rerun & vLLM Failure Investigation Log

**Action Log:**
- Attempted Tier 2 rerun (`run_arena_test.sh --tier 2`)
- vLLM (Qwen) failed to start: nanobind reference counting issue, engine core initialization failed
- Restarted vLLM with forced kill and clean launch
- vLLM loaded model weights and is now listening on port 8000
- Rerun attempted again: agent startup failed (only 2/3 agents ready)
- vLLM log shows repeated `RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}`
- No attacker/defender/green agent processes running after failure

**Next Steps:**
- Deeper investigation needed: persistent vLLM engine core failure blocks Tier 2 rerun
- Will check CUDA, memory, and vLLM compatibility issues if problem persists

---

## Document Status

**Status:** ANALYSIS COMPLETE - FINDINGS PRESENT RISK TO STAGE 3

**Confidence:** HIGH (statistical analysis on 25 battles √ó 4 components √ó 3 models = robust)

**Timeline:** Analysis complete 2026-01-15 06:15 UTC

**Next Review:** Post-component-debugging (TBD)


**Linked to:** 

---
## 2026-01-15 Investigation Plan Log: Tier 2 Rerun & vLLM Engine Failure

Searched for files matching `**/docs/04-Operations/Intent-Log/Josh/2026011[3-5]*.md`, 13 matches

Read [20260115-C-NEW-001.2a-CRITICAL-BUG-Logic-Timeouts.md](20260115-C-NEW-001.2a-CRITICAL-BUG-Logic-Timeouts.md), lines 1 to 60
Read [20260115-C-NEW-001.2-Final-Investigation-Summary.md](20260115-C-NEW-001.2-Final-Investigation-Summary.md), lines 1 to 60
Read [20260115-Session-Summary.md](20260115-Session-Summary.md), lines 1 to 60
Read [20260114-Model-Selection-for-CIS-Validation.md](20260114-Model-Selection-for-CIS-Validation.md), lines 1 to 60

Based on the reviewed documentation from 20260113‚Äì20260115, here is a focused investigation plan to resolve the persistent vLLM (Qwen) engine core failure and ensure robust Tier 2 rerun completion. This plan will be logged in the component analysis report.

---
**Investigation Plan: Tier 2 Rerun & vLLM Engine Failure**
---

### 1. Immediate Technical Checks

- **a. Hardware/Resource Audit**
  - Check GPU memory and CUDA availability (`nvidia-smi`, `nvcc --version`)
  - Confirm no zombie vLLM or CUDA processes are blocking resources

- **b. vLLM Compatibility**
  - Verify vLLM version matches Qwen model requirements (AWQ, quantization flags)
  - Check for recent vLLM or CUDA library updates that may have broken compatibility

- **c. Clean Restart**
  - Fully kill all vLLM, Python, and related processes
  - Remove any temporary or lock files in tmp or model cache directories
  - Restart vLLM with explicit logging enabled

---
### 2. Log & Error Analysis

- **a. Capture Full vLLM Startup Log**
  - Save and review vllm.log from the failed run
  - Search for nanobind, reference counting, or engine core errors

- **b. Cross-Reference Known Issues**
  - Check vLLM and Qwen GitHub issues for similar nanobind or engine core failures
  - Review any recent changes in the project‚Äôs Dockerfile or environment setup

---
### 3. Environment & Dependency Validation

- **a. Python Environment**
  - Confirm the correct Python version and virtual environment are active
  - Reinstall vLLM and Qwen dependencies if needed

- **b. Model Files**
  - Validate integrity of Qwen model files (no partial/corrupt downloads)
  - Clear and re-download model weights if corruption is suspected

---
### 4. Fallback & Escalation

- **a. Alternate Hardware**
  - If hardware issues persist, attempt to run on a backup GPU or node

- **b. Minimal Repro**
  - Attempt to start vLLM with a minimal config and a small model to isolate the issue

- **c. Escalate**
  - If all else fails, escalate to vLLM maintainers or Qwen support with full logs

---
### 5. Documentation & Logging

- **a. Log all actions, commands, and findings in this report**
- **b. Summarize root cause and resolution steps once identified**
- **c. Update the Stage 3 launch checklist with any new risks or mitigations**

---
**Next Actions:**  
Begin with hardware and process checks, then proceed stepwise through the plan above. All findings and actions will be appended to this document.

---

