 0:00–0:30 — The Problem Statement. "LLM-as-judge benchmarks have a reproducibility problem." Show actual numbers: ±0.15 variance when you ask GPT to score the same code   
  twice. Show that this means leaderboard rankings are partially noise. This is a problem Berkeley researchers care about.                                                   
                                                                                                                                                                             
  0:30–1:15 — Our Solution. Explain the CIS framework — four dimensions, each grounded in observable signals. Don't just list them, explain WHY each one is grounded the way 
  it is. T comes from actual pytest execution in a Docker sandbox — not the LLM's opinion about whether tests look good. A comes from AST constraint checking — not the LLM  
  guessing about code structure. R uses sentence embedding cosine similarity — a deterministic math operation. L is the only LLM-driven component, and it's clamped to ±0.10.
   Show the formula. Show the variance dropping to <0.05. This is the intellectual meat.                                                                                     
                                                                                                                                                                             
  1:15–1:45 — The Adversarial Architecture. Explain the Red Agent — MCTS-based vulnerability exploration for complex tasks, static analysis for simple ones, embedded inside 
  Green to dynamically adjust aggressiveness based on task complexity. Show how findings feed into the score as a multiplicative penalty. Mention the intent-code mismatch   
  detection as a concrete example of catching failure modes that pure LLM judging misses (Purple returned factorial for an LRU cache task, scored 0.60 with LLM-only, our    
  system caught it at 0.19).                                                                                                                                                 
                                                                                                                                                                             
  1:45–2:15 — Self-Improving Evaluation. Battle memory stores every evaluation in SQLite. Strategy evolver uses UCB1 to select between evaluation strategies (aggressive red,
   correctness focus, deep refinement, security hunter, fast lenient) and converges on the best one per task type over time. Task intelligence dynamically generates attack  
  hints and constraints for novel tasks using LLM — so the benchmark handles ANY coding task, not just the 20 hardcoded ones. This is where you show it's not a static       
  benchmark.                                                                                                                                                                 
                                                                                                                                                                             
  2:15–2:45 — Evidence. Show the baseline results table — 20 tasks, avg CIS 0.55, range 0.00–0.75. Point out that the score distribution tracks real task difficulty         
  (beginner tasks ~0.66, expert ~0.51, task-013 REST router scored 0.00 because Purple genuinely failed). Show one DBOM as the cryptographic audit trail. Mention            
  deterministic settings: seed=42, temperature=0.                                                                                                                            
                                                                                                                                                                             
  2:45–3:00 — Close. One sentence on what's next (prompt injection hardening, multi-model evaluation). Repo link.                                                            
                                                                                                                                                                             
  The key difference from before: instead of watching a terminal run, judges see the thinking behind the system. The demo moments should be screenshots or quick clips       
  supporting the narrative, not the narrative itself. A 3-second clip of the refinement loop output is more powerful when it's illustrating the point you just made about    
  adversarial evaluation than when it's the centerpiece of the video.